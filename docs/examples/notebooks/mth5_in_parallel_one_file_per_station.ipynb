{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating one mth5 file per station in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how to generate one mth5 file per station (with associated [mt_metadata](https://mt-metadata.readthedocs.io/en/latest/)) from a survey in parallel on HPC.\n",
    "\n",
    "For this example, we will be converting Earth Data Logger (ASCII) time series data from 93 stations of the AusLAMP Musgraves Province survey (see https://dx.doi.org/10.25914/5eaa30d63bd17). The ASCII time series were concatenated per run for each station and the total volume of time series data was 106 GB. 90 of the 93 stations were a single run - stations SA246, SA299 and SA324-2 had multiple runs. \n",
    "\n",
    "This example was tested on the [National Computational Infrastructure's Gadi HPC system](https://nci.org.au/our-systems/hpc-systems). Gadi is Australiaâ€™s most powerful supercomputer, a highly parallel cluster comprising more than 200,000 processor cores on ten different types of compute nodes. The example also makes use of the [NCI-geophysics 2022.06 module](https://opus.nci.org.au/x/1wG7CQ) which contains the Python libraries used in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our mth5_onefileperstation.py code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [mth5_in_parallel tutorial](https://github.com/kujaku11/mth5/blob/master/examples/notebooks/mth5_in_parallel.ipynb), we built a single mth5 file containing all stations in our survey. In that example, we had to construct two codes:\n",
    "1. mth5_skeleton.py which created the mth5 file structure without adding in any time series data. This code could not be run in parallel as the processes involved were collective (see `Collective versus independent operations` in https://docs.h5py.org/en/stable/mpi.html).\n",
    "2. mth5_muscle.py which added in the timeseries data for each station in parallel (i.e. one station per node).\n",
    "\n",
    "For this example, we will be creating a single file per station with associated mt_metadata. As a result, we only need to create one code as the generation of each file is independent (i.e., the different mpi ranks do not need to communicate with each other). The authors use [mpi4py](https://mpi4py.readthedocs.io/en/stable/#) for this tutorial, but note that other libraries such as [Dask](https://www.dask.org/) or [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) could also be used.  \n",
    "\n",
    "Our `mth5_onefileperstation.py` code requires the following Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import h5py\n",
    "from mth5.mth5 import MTH5\n",
    "import numpy as np\n",
    "from os import path\n",
    "import os, psutil\n",
    "import glob\n",
    "import nc_time_axis\n",
    "import time\n",
    "\n",
    "from mt_metadata import timeseries as metadata\n",
    "from mt_metadata.utils.mttime import MTime\n",
    "import json\n",
    "\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using [mpi4py](https://mpi4py.readthedocs.io/en/stable/), we will need to define the MPI communicator, rank and size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = MPI.COMM_WORLD\n",
    "rank = MPI.COMM_WORLD.rank\n",
    "size = MPI.COMM_WORLD.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define our: \n",
    "1. working directories and file paths\n",
    "2. Earth Data Logger channels\n",
    "3. survey stations\n",
    "4. survey name and run number (for stations with a single run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define working directories and file paths\n",
    "\n",
    "work_dir = '/g/data/.../.../merged_data_all'\n",
    "mth5_output_directory = '/g/data/.../.../mth5_outdir_single_file_per_station'\n",
    "full_path_to_mth5_files = sorted(glob.glob(mth5_output_directory+\"/*\")) \n",
    "full_path_to_ascii_files = sorted(glob.glob(work_dir+\"/*\"))\n",
    "mt_metadata_dir = '/g/data/.../.../mt_metadata_json'\n",
    "full_path_to_mt_metadata = sorted(glob.glob(mt_metadata_dir+\"/*\"))\n",
    "survey_file_name = 'survey.json'\n",
    "survey_json = mt_metadata_dir+'/'+survey_file_name\n",
    "\n",
    "### define raw time series data channels\n",
    "\n",
    "raw_data_channels = ['EX','EY','BX','BY','BZ','TP','ambientTemperature']\n",
    "\n",
    "\n",
    "### define the stations in our survey\n",
    "\n",
    "stations_all = ['SA225-2','SA227',   'SA242',  'SA243',  'SA245',\n",
    "                'SA247',  'SA248',   'SA249',  'SA250',  'SA251',  \n",
    "                'SA252',  'SA26W-2', 'SA270',  'SA271',  'SA272',                \n",
    "                'SA273',  'SA274-2', 'SA274',  'SA275',  'SA276',\n",
    "                'SA277',  'SA293-2', 'SA294',  'SA295',  'SA296', \n",
    "                'SA297',  'SA298',   'SA300',  'SA301',  'SA319',                            \n",
    "                'SA320',  'SA320-2', 'SA321',  'SA322',  'SA323', \n",
    "                'SA324',  'SA325-2', 'SA325',  'SA326N', 'SA326S',\n",
    "                'SA344',  'SA344-2', 'SA345',  'SA346',  'SA347',  \n",
    "                'SA348',  'SA349',   'SA350',  'SA351',             ### 49 single run SA stations\n",
    "                'WA10',   'WA13',    'WA14',   'WA15',   'WA26',\n",
    "                'WA27',   'WA29',    'WA30',   'WA31',   'WA42',\n",
    "                'WA43',   'WA44',    'WA45',   'WA46',   'WA47',\n",
    "                'WA54',   'WA55',    'WA56',   'WA57',   'WA58',\n",
    "                'WA60',   'WA61',    'WA62',   'WA63',   'WA64',\n",
    "                'WA65',   'WA66',    'WA67',   'WA68',   'WA69',\n",
    "                'WA70',   'WA71',    'WA72',   'WA73',   'WA74',\n",
    "                'WA75',   'WANT19',  'WANT38', 'WANT45', 'WASA302',  \n",
    "                'WASA327',                          ### 41 single run WA stations \n",
    "                'SA246',  'SA299',   'SA324-2']     ### 3 stations with multiple runs\n",
    "                                   \n",
    "                                      \n",
    "### define the survey name and run number (for stations with a single run)\n",
    "\n",
    "survey_name = \"AusLAMP_Musgraves\"\n",
    "run_number = \"001\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also be adding [mt_metadata](https://mt-metadata.readthedocs.io/en/latest/) into our mth5 files. For this example, a single json file was created per run and the contents of each json file looked something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{     \n",
    "     \"electric_ex\": {\n",
    "         \"channel_number\": 0,\n",
    "         \"component\": null,\n",
    "         \"data_quality.rating.value\": 0,\n",
    "         \"dipole_length\": null,\n",
    "         \"filter.applied\": [\n",
    "             false\n",
    "         ],\n",
    "         \"filter.name\": [],\n",
    "         \"measurement_azimuth\": 0.0,\n",
    "         \"measurement_tilt\": 0.0,\n",
    "         \"negative.elevation\": 0.0,\n",
    "         \"negative.id\": null,\n",
    "         \"negative.latitude\": 0.0,\n",
    "         \"negative.longitude\": 0.0,\n",
    "         \"negative.manufacturer\": null,\n",
    "         \"negative.type\": null,\n",
    "         \"positive.elevation\": 0.0,\n",
    "         \"positive.id\": null,\n",
    "         \"positive.latitude\": 0.0,\n",
    "         \"positive.longitude\": 0.0,\n",
    "         \"positive.manufacturer\": null,\n",
    "         \"positive.type\": null,\n",
    "         \"sample_rate\": 10.0,\n",
    "         \"time_period.end\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"time_period.start\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"type\": \"electric\",\n",
    "         \"units\": null\n",
    "     },\n",
    "\n",
    "     \"electric_ey\": {\n",
    "         \"channel_number\": 0,\n",
    "         \"component\": null,\n",
    "         \"data_quality.rating.value\": 0,\n",
    "         \"dipole_length\": null,\n",
    "         \"filter.applied\": [\n",
    "             false\n",
    "         ],\n",
    "         \"filter.name\": [],\n",
    "         \"measurement_azimuth\": 0.0,\n",
    "         \"measurement_tilt\": 0.0,\n",
    "         \"negative.elevation\": 0.0,\n",
    "         \"negative.id\": null,\n",
    "         \"negative.latitude\": 0.0,\n",
    "         \"negative.longitude\": 0.0,\n",
    "         \"negative.manufacturer\": null,\n",
    "         \"negative.type\": null,\n",
    "         \"positive.elevation\": 0.0,\n",
    "         \"positive.id\": null,\n",
    "         \"positive.latitude\": 0.0,\n",
    "         \"positive.longitude\": 0.0,\n",
    "         \"positive.manufacturer\": null,\n",
    "         \"positive.type\": null,\n",
    "         \"sample_rate\": 10.0,\n",
    "         \"time_period.end\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"time_period.start\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"type\": \"electric\",\n",
    "         \"units\": null\n",
    "     },\n",
    "\n",
    "\n",
    "     \"magnetic_bx\": {\n",
    "         \"channel_number\": 0,\n",
    "         \"component\": null,\n",
    "         \"data_quality.rating.value\": 0,\n",
    "         \"filter.applied\": [\n",
    "             false\n",
    "         ],\n",
    "         \"filter.name\": [],\n",
    "         \"location.elevation\": 0.0,\n",
    "         \"location.latitude\": 0.0,\n",
    "         \"location.longitude\": 0.0,\n",
    "         \"measurement_azimuth\": 0.0,\n",
    "         \"measurement_tilt\": 0.0,\n",
    "         \"sample_rate\": 10.0,\n",
    "         \"sensor.id\": null,\n",
    "         \"sensor.manufacturer\": null,\n",
    "         \"sensor.type\": null,\n",
    "         \"time_period.end\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"time_period.start\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"type\": \"magnetic\",\n",
    "         \"units\": null\n",
    "     },\n",
    "\n",
    "\n",
    "     \"magnetic_by\": {\n",
    "         \"channel_number\": 0,\n",
    "         \"component\": null,\n",
    "         \"data_quality.rating.value\": 0,\n",
    "         \"filter.applied\": [\n",
    "             false\n",
    "         ],\n",
    "         \"filter.name\": [],\n",
    "         \"location.elevation\": 0.0,\n",
    "         \"location.latitude\": 0.0,\n",
    "         \"location.longitude\": 0.0,\n",
    "         \"measurement_azimuth\": 0.0,\n",
    "         \"measurement_tilt\": 0.0,\n",
    "         \"sample_rate\": 10.0,\n",
    "         \"sensor.id\": null,\n",
    "         \"sensor.manufacturer\": null,\n",
    "         \"sensor.type\": null,\n",
    "         \"time_period.end\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"time_period.start\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"type\": \"magnetic\",\n",
    "         \"units\": null\n",
    "     },\n",
    "\n",
    "\n",
    "     \"magnetic_bz\": {\n",
    "         \"channel_number\": 0,\n",
    "         \"component\": null,\n",
    "         \"data_quality.rating.value\": 0,\n",
    "         \"filter.applied\": [\n",
    "             false\n",
    "         ],\n",
    "         \"filter.name\": [],\n",
    "         \"location.elevation\": 0.0,\n",
    "         \"location.latitude\": 0.0,\n",
    "         \"location.longitude\": 0.0,\n",
    "         \"measurement_azimuth\": 0.0,\n",
    "         \"measurement_tilt\": 0.0,\n",
    "         \"sample_rate\": 10.0,\n",
    "         \"sensor.id\": null,\n",
    "         \"sensor.manufacturer\": null,\n",
    "         \"sensor.type\": null,\n",
    "         \"time_period.end\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"time_period.start\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"type\": \"magnetic\",\n",
    "         \"units\": null\n",
    "     },\n",
    "\n",
    "\n",
    "     \"run\": {\n",
    "         \"channels_recorded_auxiliary\": [],\n",
    "         \"channels_recorded_electric\": [],\n",
    "         \"channels_recorded_magnetic\": [],\n",
    "         \"data_logger.firmware.author\": null,\n",
    "         \"data_logger.firmware.name\": null,\n",
    "         \"data_logger.firmware.version\": null,\n",
    "         \"data_logger.id\": null,\n",
    "         \"data_logger.manufacturer\": null,\n",
    "         \"data_logger.timing_system.drift\": 0.0,\n",
    "         \"data_logger.timing_system.type\": \"GPS\",\n",
    "         \"data_logger.timing_system.uncertainty\": 0.0,\n",
    "         \"data_logger.type\": null,\n",
    "         \"data_type\": \"LPMT\",\n",
    "         \"id\": null,\n",
    "         \"sample_rate\": 10.0,\n",
    "         \"time_period.end\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"time_period.start\": \"1980-01-01T00:00:00+00:00\"\n",
    "     },\n",
    "\n",
    "     \"station\": {\n",
    "         \"acquired_by.name\": null,\n",
    "         \"channels_recorded\": [],\n",
    "         \"data_type\": \"LPMT\",\n",
    "         \"geographic_name\": null,\n",
    "         \"id\": null,\n",
    "         \"location.declination.model\": \"WMM\",\n",
    "         \"location.declination.value\": 0.0,\n",
    "         \"location.elevation\": 0.0,\n",
    "         \"location.latitude\": 0.0,\n",
    "         \"location.longitude\": 0.0,\n",
    "         \"orientation.method\": null,\n",
    "         \"orientation.reference_frame\": \"geographic\",\n",
    "         \"provenance.creation_time\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"provenance.software.author\": \"none\",\n",
    "         \"provenance.software.name\": null,\n",
    "         \"provenance.software.version\": null,\n",
    "         \"provenance.submitter.email\": null,\n",
    "         \"provenance.submitter.organization\": null,\n",
    "         \"run_list\": [],\n",
    "         \"time_period.end\": \"1980-01-01T00:00:00+00:00\",\n",
    "         \"time_period.start\": \"1980-01-01T00:00:00+00:00\"\n",
    "     },\n",
    "\n",
    "     \"survey\": {\n",
    "         \"citation_dataset.doi\": null,\n",
    "         \"citation_journal.doi\": null,\n",
    "         \"country\": null,\n",
    "         \"datum\": \"WGS84\",\n",
    "         \"geographic_name\": null,\n",
    "         \"id\": null,\n",
    "         \"name\": null,\n",
    "         \"northwest_corner.latitude\": 0.0,\n",
    "         \"northwest_corner.longitude\": 0.0,\n",
    "         \"project\": null,\n",
    "         \"project_lead.email\": null,\n",
    "         \"project_lead.organization\": null,\n",
    "         \"release_license\": \"CC-0\",\n",
    "         \"southeast_corner.latitude\": 0.0,\n",
    "         \"southeast_corner.longitude\": 0.0,\n",
    "         \"summary\": null,\n",
    "         \"time_period.end_date\": \"1980-01-01\",\n",
    "         \"time_period.start_date\": \"1980-01-01\"\n",
    "     },\n",
    " }\n",
    " ```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the mt_metadata in these json files were synthetic and were mainly used to show how mt_metadata could be added into our automation. \n",
    "\n",
    "We will now define functions that will be used to create our MTH5 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mth5_dir(mth5_output_directory):\n",
    "### creates the mth5 output directory if it doesn't already exist\n",
    "    try:\n",
    "        os.makedirs(mth5_output_directory)\n",
    "    except FileExistsError:\n",
    "        # directory already exists\n",
    "        print('directory already exists!')\n",
    "        pass\n",
    "\n",
    "\n",
    "def remove_existing_mth5_files(mth5_files):\n",
    "### removes existing mth5 files if they exists\n",
    "    for mth5_file in mth5_files:\n",
    "        if path.exists(mth5_file):\n",
    "            os.unlink(mth5_file)\n",
    "            print(\"INFO: Removed existing file {}\".format(mth5_file))\n",
    "        else:\n",
    "            print(\"File does not exist\")\n",
    "\n",
    "\n",
    "def channel_data_extraction(channels):\n",
    "### extracts electromagnetic time series data from concatenated (per run) Earth Data Logger \n",
    "### ASCII time series files\n",
    "    EX = [file for file in channels if file.endswith('EX')]\n",
    "    EY = [file for file in channels if file.endswith('EY')]\n",
    "    BX = [file for file in channels if file.endswith('BX')]\n",
    "    BY = [file for file in channels if file.endswith('BY')]\n",
    "    BZ = [file for file in channels if file.endswith('BZ')]\n",
    "    with open(EX[0], 'r') as file:\n",
    "        EX1 = file.read().splitlines()\n",
    "        ex_ts = np.array(EX1).astype(np.int32)\n",
    "    with open(EY[0], 'r') as file:\n",
    "        EY1 = file.read().splitlines()\n",
    "        ey_ts = np.array(EY1).astype(np.int32)\n",
    "    with open(BX[0], 'r') as file:\n",
    "        BX1 = file.read().splitlines()\n",
    "        bx_ts = np.array(BX1).astype(np.int32)\n",
    "    with open(BY[0], 'r') as file:\n",
    "        BY1 = file.read().splitlines()\n",
    "        by_ts = np.array(BY1).astype(np.int32)\n",
    "    with open(BZ[0], 'r') as file:\n",
    "        BZ1 = file.read().splitlines()\n",
    "        bz_ts = np.array(BZ1).astype(np.int32)\n",
    "\n",
    "    return ex_ts, ey_ts, bx_ts, by_ts, bz_ts\n",
    "\n",
    "\n",
    "def create_mth5_group_station_run_channel(station,mt_metadata):\n",
    "### creates the mth5 gropus, stations, runs and channels for each mth5 file \n",
    "### and populates mt_metadata from the relevant json file\n",
    "    with open(mt_metadata[0], 'r') as json_file:\n",
    "        json_load = json.load(json_file)\n",
    "   \n",
    "    station_dict = json_load['station']\n",
    "    add_station = m.add_station(station, survey=survey_name)\n",
    "    add_station.metadata.from_dict(station_dict)\n",
    "    add_station.write_metadata()\n",
    "\n",
    "    channels = []\n",
    "    for file in full_path_to_ascii_files:\n",
    "        if station in file:\n",
    "            channels.append(file)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "### for stations with a single run:            \n",
    "    if len(channels) == len(raw_data_channels):\n",
    "        run_dict = json_load['run']\n",
    "        ex_dict = json_load['electric_ex']\n",
    "        ey_dict = json_load['electric_ey']\n",
    "        bx_dict = json_load['magnetic_bx']\n",
    "        by_dict = json_load['magnetic_by']\n",
    "        bz_dict = json_load['magnetic_bz']\n",
    "        \n",
    "        add_run = m.add_run(station, run_number, survey=survey_name) \n",
    "        add_run.metadata.from_dict(run_dict)\n",
    "        add_run.write_metadata()\n",
    " \n",
    "        ex_ts,ey_ts,bx_ts,by_ts,bz_ts = channel_data_extraction(channels)\n",
    "\n",
    "        ex = m.add_channel(station, run_number, \"ex\", \"electric\", ex_ts, survey=survey_name)\n",
    "        ey = m.add_channel(station, run_number, \"ey\", \"electric\", ey_ts, survey=survey_name)\n",
    "        bx = m.add_channel(station, run_number, \"bx\", \"magnetic\", bx_ts, survey=survey_name)\n",
    "        by = m.add_channel(station, run_number, \"by\", \"magnetic\", by_ts, survey=survey_name)\n",
    "        bz = m.add_channel(station, run_number, \"bz\", \"magnetic\", bz_ts, survey=survey_name)\n",
    "              \n",
    "        ex.metadata.from_dict(ex_dict)\n",
    "        ex.write_metadata()\n",
    "        ey.metadata.from_dict(ey_dict)\n",
    "        ey.write_metadata()\n",
    "        bx.metadata.from_dict(bx_dict)\n",
    "        bx.write_metadata()\n",
    "        by.metadata.from_dict(by_dict)\n",
    "        by.write_metadata()\n",
    "        bz.metadata.from_dict(bz_dict)\n",
    "        bz.write_metadata()\n",
    "        \n",
    "### for stations with multiple runs:\n",
    "\n",
    "        elif len(channels) > len(raw_data_channels):\n",
    "        sort_files = sorted(channels)\n",
    "        number_of_channels = len(raw_data_channels)\n",
    "        split_lists = [sort_files[x:x+number_of_channels] for x in range(0, len(sort_files), number_of_channels)]\n",
    "        mt_metadata_files = sorted(mt_metadata)\n",
    "        for i, (group,mt_meta) in enumerate(zip(split_lists,mt_metadata_files)):\n",
    "            mrun_number = i+1\n",
    "            run = \"00%i\" % mrun_number\n",
    "            with open(mt_meta, 'r') as json_file:\n",
    "                json_load = json.load(json_file)\n",
    "            \n",
    "            run_dict = json_load['run']\n",
    "            ex_dict = json_load['electric_ex']\n",
    "            ey_dict = json_load['electric_ey']\n",
    "            bx_dict = json_load['magnetic_bx']\n",
    "            by_dict = json_load['magnetic_by']\n",
    "            bz_dict = json_load['magnetic_bz']\n",
    "\n",
    "            add_run = m.add_run(station, run, survey=survey_name)            \n",
    "            add_run.metadata.from_dict(run_dict)\n",
    "            add_run.write_metadata()\n",
    "\n",
    "            ex_ts,ey_ts,bx_ts,by_ts,bz_ts = channel_data_extraction(channels)\n",
    "\n",
    "            ex = m.add_channel(station, run, \"ex\", \"electric\", ex_ts, survey=survey_name)\n",
    "            ey = m.add_channel(station, run, \"ey\", \"electric\", ey_ts, survey=survey_name)\n",
    "            bx = m.add_channel(station, run, \"bx\", \"magnetic\", bx_ts, survey=survey_name)\n",
    "            by = m.add_channel(station, run, \"by\", \"magnetic\", by_ts, survey=survey_name)\n",
    "            bz = m.add_channel(station, run, \"bz\", \"magnetic\", bz_ts, survey=survey_name)\n",
    "            \n",
    "            ex.metadata.from_dict(ex_dict)\n",
    "            ex.write_metadata()\n",
    "            ey.metadata.from_dict(ey_dict)\n",
    "            ey.write_metadata()\n",
    "            bx.metadata.from_dict(bx_dict)\n",
    "            bx.write_metadata()\n",
    "            by.metadata.from_dict(by_dict)\n",
    "            by.write_metadata()\n",
    "            bz.metadata.from_dict(bz_dict)\n",
    "            bz.write_metadata()\n",
    "\n",
    "    elif len(channels) < len(raw_data_channels):\n",
    "        print('you are likely missing some channels')\n",
    "        print(station)\n",
    "\n",
    "    else:\n",
    "        print('something has gone wrong')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to create our mth5 files per station. For this we will be creating a single mth5 file per MPI rank for each station in our survey. As each process on each rank is independent, we can run all processes in parallel. Additionally, we can add compression to each file as we are not using the Parallel HDF5 library (which doesn't support compression) that was used in the [mth5_in_parallel tutorial](https://github.com/kujaku11/mth5/blob/master/examples/notebooks/mth5_in_parallel.ipynb). For this example, we will use \"gzip\" level 4 compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create mth5 output directory and remove existing files \n",
    "\n",
    "if rank==0:\n",
    "    make_mth5_dir(mth5_output_directory)\n",
    "    remove_existing_mth5_files(full_path_to_mth5_files)\n",
    "\n",
    "comm.Barrier()\n",
    "\n",
    "### create a single mth5 file per station with compression and associated mt_metadata \n",
    "\n",
    "for i,station in enumerate(sorted(stations_all)):\n",
    "    if i%size!=rank:\n",
    "        continue\n",
    "    m = MTH5(file_version='0.2.0',shuffle=None,fletcher32=None,compression=\"gzip\",compression_opts=4)\n",
    "    hdf5_filename = '{}.h5'.format(station)\n",
    "    h5_fn = mth5_output_directory+'/'+hdf5_filename \n",
    "    m.open_mth5(h5_fn, \"w\") \n",
    "    survey_group = m.add_survey(survey_name)\n",
    "    with open(survey_json, 'r') as json_file:\n",
    "        json_load = json.load(json_file)\n",
    "    survey_dict = json_load['survey']\n",
    "    survey_group.metadata.from_dict(survey_dict)\n",
    "    survey_group.write_metadata()    \n",
    "    mt_metadata_file_name = '{}.json'.format(station)\n",
    "    mt_metadata = [file for file in full_path_to_mt_metadata if file.endswith(mt_metadata_file_name)]\n",
    "    create_mth5_group_station_run_channel(station,mt_metadata)\n",
    "    m.close_mth5()\n",
    "\n",
    "\n",
    "comm.Barrier()\n",
    "\n",
    "### print total time to run script\n",
    "if rank==0:\n",
    "    print('The script took {0} seconds !'.format(time.time()-startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this altogether into a Python script (`mth5_onefileperstation.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import h5py\n",
    "from mth5.mth5 import MTH5\n",
    "import numpy as np\n",
    "from os import path\n",
    "import os, psutil\n",
    "import glob\n",
    "import nc_time_axis\n",
    "import time\n",
    "\n",
    "from mt_metadata import timeseries as metadata\n",
    "from mt_metadata.utils.mttime import MTime\n",
    "import json\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "### define MPI comm, rank and size\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = MPI.COMM_WORLD.rank\n",
    "size = MPI.COMM_WORLD.size\n",
    "\n",
    "\n",
    "\n",
    "### define working directories and file paths\n",
    "\n",
    "work_dir = '/g/data/.../.../merged_data_all'\n",
    "mth5_output_directory = '/g/data/.../.../mth5_outdir_single_file_per_station'\n",
    "full_path_to_mth5_files = sorted(glob.glob(mth5_output_directory+\"/*\")) \n",
    "full_path_to_ascii_files = sorted(glob.glob(work_dir+\"/*\"))\n",
    "mt_metadata_dir = '/g/data/.../.../mt_metadata_json'\n",
    "full_path_to_mt_metadata = sorted(glob.glob(mt_metadata_dir+\"/*\"))\n",
    "survey_file_name = 'survey.json'\n",
    "survey_json = mt_metadata_dir+'/'+survey_file_name\n",
    "\n",
    "### define raw time series data channels\n",
    "\n",
    "raw_data_channels = ['EX','EY','BX','BY','BZ','TP','ambientTemperature']\n",
    "\n",
    "\n",
    "### define stations to go into mth5 file\n",
    "\n",
    "stations_all = ['SA225-2','SA227',   'SA242',  'SA243',  'SA245',\n",
    "                'SA247',  'SA248',   'SA249',  'SA250',  'SA251',  \n",
    "                'SA252',  'SA26W-2', 'SA270',  'SA271',  'SA272',                \n",
    "                'SA273',  'SA274-2', 'SA274',  'SA275',  'SA276',\n",
    "                'SA277',  'SA293-2', 'SA294',  'SA295',  'SA296', \n",
    "                'SA297',  'SA298',   'SA300',  'SA301',  'SA319',                            \n",
    "                'SA320',  'SA320-2', 'SA321',  'SA322',  'SA323', \n",
    "                'SA324',  'SA325-2', 'SA325',  'SA326N', 'SA326S',\n",
    "                'SA344',  'SA344-2', 'SA345',  'SA346',  'SA347',  \n",
    "                'SA348',  'SA349',   'SA350',  'SA351',             ### 49 single run SA stations\n",
    "                'WA10',   'WA13',    'WA14',   'WA15',   'WA26',\n",
    "                'WA27',   'WA29',    'WA30',   'WA31',   'WA42',\n",
    "                'WA43',   'WA44',    'WA45',   'WA46',   'WA47',\n",
    "                'WA54',   'WA55',    'WA56',   'WA57',   'WA58',\n",
    "                'WA60',   'WA61',    'WA62',   'WA63',   'WA64',\n",
    "                'WA65',   'WA66',    'WA67',   'WA68',   'WA69',\n",
    "                'WA70',   'WA71',    'WA72',   'WA73',   'WA74',\n",
    "                'WA75',   'WANT19',  'WANT38', 'WANT45', 'WASA302',  \n",
    "                'WASA327',                          ### 41 single run WA stations \n",
    "                'SA246',  'SA299',   'SA324-2']     ### 3 stations with multiple runs\n",
    "                                   \n",
    "                                      \n",
    "### define survey name and run number (for stations with a single run)\n",
    "\n",
    "survey_name = \"AusLAMP_Musgraves\"\n",
    "run_number = \"001\"\n",
    "\n",
    "\n",
    "### define functions\n",
    "\n",
    "def make_mth5_dir(mth5_output_directory):\n",
    "### creates mth5 output directory if it doesn't already exist\n",
    "    try:\n",
    "        os.makedirs(mth5_output_directory)\n",
    "    except FileExistsError:\n",
    "        # directory already exists\n",
    "        print('directory already exists!')\n",
    "        pass\n",
    "\n",
    "\n",
    "def remove_existing_mth5_files(mth5_files):\n",
    "### removes existing mth5 files if they exists\n",
    "    for mth5_file in mth5_files:\n",
    "        if path.exists(mth5_file):\n",
    "            os.unlink(mth5_file)\n",
    "            print(\"INFO: Removed existing file {}\".format(mth5_file))\n",
    "        else:\n",
    "            print(\"File does not exist\")\n",
    "\n",
    "\n",
    "def channel_data_extraction(channels):\n",
    "### extracts electromagnetic time series data from concatenated (per run) Earth Data Logger ASCII time series files\n",
    "    EX = [file for file in channels if file.endswith('EX')]\n",
    "    EY = [file for file in channels if file.endswith('EY')]\n",
    "    BX = [file for file in channels if file.endswith('BX')]\n",
    "    BY = [file for file in channels if file.endswith('BY')]\n",
    "    BZ = [file for file in channels if file.endswith('BZ')]\n",
    "    with open(EX[0], 'r') as file:\n",
    "        EX1 = file.read().splitlines()\n",
    "        ex_ts = np.array(EX1).astype(np.int32)\n",
    "    with open(EY[0], 'r') as file:\n",
    "        EY1 = file.read().splitlines()\n",
    "        ey_ts = np.array(EY1).astype(np.int32)\n",
    "    with open(BX[0], 'r') as file:\n",
    "        BX1 = file.read().splitlines()\n",
    "        bx_ts = np.array(BX1).astype(np.int32)\n",
    "    with open(BY[0], 'r') as file:\n",
    "        BY1 = file.read().splitlines()\n",
    "        by_ts = np.array(BY1).astype(np.int32)\n",
    "    with open(BZ[0], 'r') as file:\n",
    "        BZ1 = file.read().splitlines()\n",
    "        bz_ts = np.array(BZ1).astype(np.int32)\n",
    "\n",
    "    return ex_ts, ey_ts, bx_ts, by_ts, bz_ts\n",
    "\n",
    "\n",
    "def create_mth5_group_station_run_channel(station,mt_metadata):\n",
    "### creates the mth5 gropus, stations, runs and channels for each mth5 file \n",
    "### and populates mt_metadata from the relevant json file\n",
    "    with open(mt_metadata[0], 'r') as json_file:\n",
    "        json_load = json.load(json_file)\n",
    "   \n",
    "    station_dict = json_load['station']\n",
    "    add_station = m.add_station(station, survey=survey_name)\n",
    "    add_station.metadata.from_dict(station_dict)\n",
    "    add_station.write_metadata()\n",
    "\n",
    "    channels = []\n",
    "    for file in full_path_to_ascii_files:\n",
    "        if station in file:\n",
    "            channels.append(file)\n",
    "        else:\n",
    "            continue\n",
    "### for stations with a single run:            \n",
    "    if len(channels) == len(raw_data_channels):\n",
    "        run_dict = json_load['run']\n",
    "        ex_dict = json_load['electric_ex']\n",
    "        ey_dict = json_load['electric_ey']\n",
    "        bx_dict = json_load['magnetic_bx']\n",
    "        by_dict = json_load['magnetic_by']\n",
    "        bz_dict = json_load['magnetic_bz']\n",
    "        \n",
    "        add_run = m.add_run(station, run_number, survey=survey_name) \n",
    "        add_run.metadata.from_dict(run_dict)\n",
    "        add_run.write_metadata()\n",
    " \n",
    "        ex_ts,ey_ts,bx_ts,by_ts,bz_ts = channel_data_extraction(channels)\n",
    "\n",
    "        ex = m.add_channel(station, run_number, \"ex\", \"electric\", ex_ts, survey=survey_name)\n",
    "        ey = m.add_channel(station, run_number, \"ey\", \"electric\", ey_ts, survey=survey_name)\n",
    "        bx = m.add_channel(station, run_number, \"bx\", \"magnetic\", bx_ts, survey=survey_name)\n",
    "        by = m.add_channel(station, run_number, \"by\", \"magnetic\", by_ts, survey=survey_name)\n",
    "        bz = m.add_channel(station, run_number, \"bz\", \"magnetic\", bz_ts, survey=survey_name)\n",
    "                \n",
    "        ex.metadata.from_dict(ex_dict)\n",
    "        ex.write_metadata()\n",
    "        ey.metadata.from_dict(ey_dict)\n",
    "        ey.write_metadata()\n",
    "        bx.metadata.from_dict(bx_dict)\n",
    "        bx.write_metadata()\n",
    "        by.metadata.from_dict(by_dict)\n",
    "        by.write_metadata()\n",
    "        bz.metadata.from_dict(bz_dict)\n",
    "        bz.write_metadata()\n",
    "\n",
    " ### for stations with multiple runs:       \n",
    "    elif len(channels) > len(raw_data_channels):\n",
    "        sort_files = sorted(channels)\n",
    "        number_of_channels = len(raw_data_channels)\n",
    "        split_lists = [sort_files[x:x+number_of_channels] for x in range(0, len(sort_files), number_of_channels)]\n",
    "        mt_metadata_files = sorted(mt_metadata)\n",
    "        for i, (group,mt_meta) in enumerate(zip(split_lists,mt_metadata_files)):\n",
    "            mrun_number = i+1\n",
    "            run = \"00%i\" % mrun_number\n",
    "            with open(mt_meta, 'r') as json_file:\n",
    "                json_load = json.load(json_file)\n",
    "            \n",
    "            run_dict = json_load['run']\n",
    "            ex_dict = json_load['electric_ex']\n",
    "            ey_dict = json_load['electric_ey']\n",
    "            bx_dict = json_load['magnetic_bx']\n",
    "            by_dict = json_load['magnetic_by']\n",
    "            bz_dict = json_load['magnetic_bz']\n",
    "\n",
    "            add_run = m.add_run(station, run, survey=survey_name)            \n",
    "            add_run.metadata.from_dict(run_dict)\n",
    "            add_run.write_metadata()\n",
    "\n",
    "            ex_ts,ey_ts,bx_ts,by_ts,bz_ts = channel_data_extraction(channels)\n",
    "\n",
    "            ex = m.add_channel(station, run, \"ex\", \"electric\", ex_ts, survey=survey_name)\n",
    "            ey = m.add_channel(station, run, \"ey\", \"electric\", ey_ts, survey=survey_name)\n",
    "            bx = m.add_channel(station, run, \"bx\", \"magnetic\", bx_ts, survey=survey_name)\n",
    "            by = m.add_channel(station, run, \"by\", \"magnetic\", by_ts, survey=survey_name)\n",
    "            bz = m.add_channel(station, run, \"bz\", \"magnetic\", bz_ts, survey=survey_name)\n",
    "\n",
    "            ex.metadata.from_dict(ex_dict)\n",
    "            ex.write_metadata()\n",
    "            ey.metadata.from_dict(ey_dict)\n",
    "            ey.write_metadata()\n",
    "            bx.metadata.from_dict(bx_dict)\n",
    "            bx.write_metadata()\n",
    "            by.metadata.from_dict(by_dict)\n",
    "            by.write_metadata()\n",
    "            bz.metadata.from_dict(bz_dict)\n",
    "            bz.write_metadata()\n",
    "\n",
    "    elif len(channels) < len(raw_data_channels):\n",
    "        print('you are likely missing some channels')\n",
    "        print(station)\n",
    "\n",
    "    else:\n",
    "        print('something has gone wrong')\n",
    "\n",
    "\n",
    "### create mth5 output directory and remove existing files \n",
    "\n",
    "if rank==0:\n",
    "    make_mth5_dir(mth5_output_directory)\n",
    "    remove_existing_mth5_files(full_path_to_mth5_files)\n",
    "\n",
    "comm.Barrier()\n",
    "\n",
    "### create a single mth5 file per station with compression and associated mt_metadata \n",
    "\n",
    "for i,station in enumerate(sorted(stations_all)):\n",
    "    if i%size!=rank:\n",
    "        continue\n",
    "    m = MTH5(file_version='0.2.0',shuffle=None,fletcher32=None,compression=\"gzip\",compression_opts=4)\n",
    "    hdf5_filename = '{}.h5'.format(station)\n",
    "    h5_fn = mth5_output_directory+'/'+hdf5_filename \n",
    "    m.open_mth5(h5_fn, \"w\") \n",
    "    survey_group = m.add_survey(survey_name)\n",
    "    with open(survey_json, 'r') as json_file:\n",
    "        json_load = json.load(json_file)\n",
    "    survey_dict = json_load['survey']\n",
    "    survey_group.metadata.from_dict(survey_dict)\n",
    "    survey_group.write_metadata()    \n",
    "    mt_metadata_file_name = '{}.json'.format(station)\n",
    "    mt_metadata = [file for file in full_path_to_mt_metadata if file.endswith(mt_metadata_file_name)]\n",
    "    create_mth5_group_station_run_channel(station,mt_metadata)\n",
    "    m.close_mth5()\n",
    "\n",
    "\n",
    "comm.Barrier()\n",
    "\n",
    "### print total time to run script\n",
    "if rank==0:\n",
    "    print('The script took {0} seconds !'.format(time.time()-startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our `mth5_onefileperstation.py` script, we next need to make a job submission script to submit to the Gadi PBSPro scheduler. The job submission script specifys the queue to use and the duration/resources needed for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "#PBS -N mth5_onefileperstation\n",
    "#PBS -q hugemem\n",
    "#PBS -P fp0\n",
    "#PBS -l walltime=0:05:00\n",
    "#PBS -l ncpus=96\n",
    "#PBS -l mem=900GB\n",
    "#PBS -l jobfs=10GB\n",
    "#PBS -l storage=gdata/fp0+gdata/my80+gdata/lm70+gdata/up99\n",
    "\n",
    "module use /g/data/up99/modulefiles\n",
    "module load NCI-geophys/22.06 \n",
    "\n",
    "cd ${PBS_O_WORKDIR}\n",
    "\n",
    "mpirun -np $PBS_NCPUS python3 mth5_onefileperstation.py > ./pbs_job_logs/$PBS_JOBID.log\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our jobscript above (`mth5_onefileperstation.sh`), we have requested:\n",
    "1. to use the [hugemem queue](https://opus.nci.org.au/x/1wBiBQ)\n",
    "2. 96 CPUs (2 nodes)\n",
    "3. 900GB of memory\n",
    "4. 5 minutes of walltime\n",
    "\n",
    "We also need to define the NCI project codes used in `mth5_onefileperstation.py`. The project code `up99` is required to make use of the NCI-geophys/22.06 module that contains the Python libraries used in this tutorial.\n",
    "\n",
    "To submit this jobscript (`mth5_onefileperstation.sh`) on Gadi:\n",
    "\n",
    "```\n",
    "$ qsub mth5_muscle.sh\n",
    "\n",
    "```\n",
    "\n",
    "This job will process the 93 stations in our survey using 96 MPI ranks (one station per rank) and creates the following 93 mth5 files:\n",
    "\n",
    "```\n",
    "SA225-2.h5  SA252.h5    SA293-2.h5  SA320.h5    SA344.h5  WA15.h5  WA47.h5  WA65.h5  WANT19.h5\n",
    "SA227.h5    SA26W-2.h5  SA294.h5    SA321.h5    SA345.h5  WA26.h5  WA54.h5  WA66.h5  WANT38.h5\n",
    "SA242.h5    SA270.h5    SA295.h5    SA322.h5    SA346.h5  WA27.h5  WA55.h5  WA67.h5  WANT45.h5\n",
    "SA243.h5    SA271.h5    SA296.h5    SA323.h5    SA347.h5  WA29.h5  WA56.h5  WA68.h5  WASA302.h5\n",
    "SA245.h5    SA272.h5    SA297.h5    SA324-2.h5  SA348.h5  WA30.h5  WA57.h5  WA69.h5  WASA327.h5\n",
    "SA246.h5    SA273.h5    SA298.h5    SA324.h5    SA349.h5  WA31.h5  WA58.h5  WA70.h5\n",
    "SA247.h5    SA274-2.h5  SA299.h5    SA325-2.h5  SA350.h5  WA42.h5  WA60.h5  WA71.h5\n",
    "SA248.h5    SA274.h5    SA300.h5    SA325.h5    SA351.h5  WA43.h5  WA61.h5  WA72.h5\n",
    "SA249.h5    SA275.h5    SA301.h5    SA326N.h5   WA10.h5   WA44.h5  WA62.h5  WA73.h5\n",
    "SA250.h5    SA276.h5    SA319.h5    SA326S.h5   WA13.h5   WA45.h5  WA63.h5  WA74.h5\n",
    "SA251.h5    SA277.h5    SA320-2.h5  SA344-2.h5  WA14.h5   WA46.h5  WA64.h5  WA75.h5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a summary of how long `mth5_onefileperstation.py` took to run for the AusLAMP Musgraves Province survey:\n",
    "\n",
    "```\n",
    "run on: Gadi\n",
    "number of stations: 93\n",
    "number of runs: 101\n",
    "NCPUs used: 96 (2 nodes)\n",
    "memory used: 791 GB\n",
    "walltime used: 215 seconds (3m35s)\n",
    "CPU time used: 48570 seconds (5h06m07s)\n",
    "service units: 17.20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have generated 93 mth5 files for the 93 stations from the Musgraves survey in 3 minutes and 35 seconds using 2 nodes (96 CPUs) on Gadi. For comparison, if we only used one CPU it would have taken approximately 5 hours to create these files.\n",
    "\n",
    "In our [mth5_in_parallel](https://github.com/kujaku11/mth5/blob/master/examples/notebooks/mth5_in_parallel.ipynb) example, we generated a single mth5 file with all stations and this took approximately 35 minutes of walltime (or ~14 hours of CPU time). The _\"single file per station\"_ model was much quicker as it did not require the creation of an mth5 skeleton as each process (rank) was run independently. \n",
    "\n",
    "We were able to introduce compression into our `mth5_onefileperstation.py` code and the total volume of the 93 mth5 files was 14 GB. The `mth5_in_parallel.ipynb` tutorial was following the _\"all stations in a single mth5 file\"_ model and as our version of Parallel HDF5 did not support compression, the total volume of the single mth5 file was 106 GB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
